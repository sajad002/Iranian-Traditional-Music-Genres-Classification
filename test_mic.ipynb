{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use(\"GTK4Agg\")\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import matplotlib.rcsetup as rcsetup\n",
    "# print(rcsetup.all_backends)\n",
    "# print(matplotlib.get_backend())\n",
    "# plt.plot(range(10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 16:38:09.956914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-20 16:38:10.782419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "print(matplotlib.get_backend())\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# record and play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration, sample_rate=22050):\n",
    "    print(\"Recording audio...\")\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()  # Wait for recording to finish\n",
    "    print(\"Recording finished.\")\n",
    "\n",
    "    # Convert the audio data to a numpy array\n",
    "    audio_data_np = audio_data.squeeze()\n",
    "    return audio_data, audio_data_np\n",
    "\n",
    "def play_audio(audio_data, sample_rate=22050):\n",
    "    print(\"Playing audio...\")\n",
    "    sd.play(audio_data, samplerate=sample_rate)\n",
    "    sd.wait()  # Wait for playback to finish\n",
    "    print(\"Playback finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# signal augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audio_data, window_duration=3, overlap=1.5, desired_sr=22050):\n",
    "\n",
    "    # Calculate the number of samples for each window\n",
    "    window_size = int(window_duration * desired_sr)\n",
    "\n",
    "    # Calculate the shift size of windows\n",
    "    shift_size = int(desired_sr * (window_duration - overlap))\n",
    "\n",
    "    # Split the audio into windows with overlap\n",
    "    windows = []\n",
    "    start = 0\n",
    "    while start + window_size <= len(audio_data):\n",
    "        windows.append(audio_data[start:start + window_size])\n",
    "        start += shift_size\n",
    "\n",
    "    # Convert the list of windows to a numpy array\n",
    "    windows = np.array(windows)\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing on input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting numpy to MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal2MFCC(windows, desired_sr=22050):\n",
    "    \n",
    "    windows = windows.astype(np.float32)\n",
    "    mfcc_features = []\n",
    "    \n",
    "    for window in windows:\n",
    "        mfcc = librosa.feature.mfcc(y=window, sr=desired_sr, n_mfcc=13)\n",
    "        mfcc_features.append(mfcc)\n",
    "\n",
    "    mfcc_features = np.array(mfcc_features)\n",
    "    return mfcc_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_input(X_test_mfccs):\n",
    "  X_test_mfcc_reshaped = X_test_mfccs.reshape(X_test_mfccs.shape[0], X_test_mfccs.shape[1], X_test_mfccs.shape[2], 1)\n",
    "\n",
    "  # print(X_test_mfccs.shape , \"reshaped: \", X_test_mfcc_reshaped.shape)\n",
    "  return  X_test_mfcc_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doing all preprocess phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_input(audio_data_np, window_duration=3, overlap=1.5, desired_sr=22050):\n",
    "    windows = augment_audio(audio_data_np, window_duration=window_duration, \\\n",
    "                            overlap=overlap, desired_sr=desired_sr)\n",
    "    mfcc_windows = signal2MFCC(windows, desired_sr=desired_sr)\n",
    "    mfcc_windows_reshaped = reshape_input(mfcc_windows)\n",
    "    return mfcc_windows_reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predidting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, mfccs, class_labels, do_print=False):\n",
    "    # Predict the genre\n",
    "    predictions = model.predict(mfccs)\n",
    "\n",
    "    # Get the predicted genre label\n",
    "    genre_label = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Convert genre label to the actual genre name (assuming you have a list of genre names)\n",
    "    predicted_genre = [class_labels[index] for index in genre_label]\n",
    "\n",
    "    if do_print : print(predicted_genre)\n",
    "    return predicted_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calling methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording audio...\n",
      "Recording finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 16:41:12.453963: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step\n",
      "['D-Shur', 'D-Shur', 'D-Shur', 'D-Shur', 'D-Shur']\n"
     ]
    }
   ],
   "source": [
    "# Set the duration and sample rate for recording\n",
    "duration = 10  # Duration in seconds\n",
    "window_duration=3\n",
    "overlap=1.5 \n",
    "desired_sr=22050\n",
    "class_labels = ['A-Abuata',         #0\n",
    "                'A-Afshari',        #1\n",
    "                'A-Bayat-E Kord',   #2\n",
    "                'A-Bayat-E Tork',   #3\n",
    "                'A-Bayat-e Esfahan',#4\n",
    "                'A-Dashti',         #5\n",
    "                'D-Chahargah',      #6\n",
    "                'D-Homayun',        #7\n",
    "                'D-Mahoor',         #8\n",
    "                'D-Nava',           #9\n",
    "                'D-Rastpanjgah',    #10\n",
    "                'D-Segah',          #11\n",
    "                'D-Shur'            #12\n",
    "                ]   \n",
    "\n",
    "# Record audio\n",
    "input_label = 0\n",
    "audio_data, audio_data_np = record_audio(duration)\n",
    "\n",
    "# Play the recorded audio\n",
    "# play_audio(audio_data)\n",
    "\n",
    "final_data = preprocessing_input(audio_data_np, window_duration=window_duration\\\n",
    "                                 ,overlap=overlap, desired_sr=desired_sr)\n",
    "\n",
    "# y_train = np.full((final_data.shape)[0], input_label)\n",
    "# num_classes = 13  # Number of classes\n",
    "# y_train_1hot = to_categorical(y_train, num_classes=num_classes)\n",
    "\n",
    "model = load_model('model-1.h5')\n",
    "\n",
    "predicted_genre = test_model(model, final_data, class_labels, do_print=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
